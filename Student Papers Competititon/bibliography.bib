
@article{sanzharov_licence_2025,
	title = {Licence Plate Recognition and Vehicle Identification {US}-{ING} Deep Learning},
	volume = {30},
	issn = {27101673, 27101681},
	url = {https://jai.in.ua/index.php/en/issues?paper_num=1699},
	doi = {10.15407/jai2025.03.103},
	abstract = {This paper presents a methodology for automated licence plate recognition and vehicle identification designed for Ukrainian standards. The approach combines {YOLOv}11 object detection with {EasyOCR} text recognition, incorporating deskewing and adaptive thresholding preprocessing techniques. A custom dataset of 467 images containing Ukrainian {DSTU}-compliant licence plates was created from the Auto. {RIA} platform. The system employs format-specific error correction and character substitution rules. Experimental results demonstrate an F‚ÇÅ score of 0.92 at {IoU}@0.5 for detection tasks, with performance variations across vehicle classes revealing challenges in commercial vehicle recognition and motorcycle licence plate processing. This work contributes annotated dataset for Ukrainian licence plate recognition and provides a practical foundation for automated traffic monitoring systems.},
	pages = {103--109},
	issue = {{AI}.2025.30(3)},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Stuc.intelekt},
	author = {Sanzharov, Daniil and Filimonova, Tatiana},
	urldate = {2026-02-08},
	date = {2025-09-30},
	langid = {english},
	file = {PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\Z99CQMH3\\State University of Trade and Economics, Ukraine et al. - 2025 - Licence Plate Recognition and Vehicle Identification US-ING Deep Learning.pdf:application/pdf},
}

@article{__2022,
	title = {–†–û–ó–ü–Ü–ó–ù–ê–í–ê–ù–ù–Ø –û–ë‚Äô–Ñ–ö–¢–Ü–í –ó–ê –î–û–ü–û–ú–û–ì–û–Æ –¢–ï–•–ù–û–õ–û–ì–Ü–ô –ö–û–ú–ü‚Äô–Æ–¢–ï–†–ù–û–ì–û –ó–û–†–£},
	issn = {2412-4338},
	url = {https://tit.dut.edu.ua/index.php/telecommunication/article/view/2443},
	abstract = {–ê–Ω–æ—Ç–∞—Ü—ñ—è
					–í–∏—è–≤–ª–µ–Ω–Ω—è –æ–±‚Äô—î–∫—Ç—ñ–≤ –¥–ª—è –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É —î –æ–¥–Ω–∏–º –∑ –∫–ª—é—á–æ–≤–∏—Ö —Ñ–∞–∫—Ç–æ—Ä—ñ–≤ –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è –ø–æ–¥—ñ–π. –¢–æ—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ–±‚Äô—î–∫—Ç–∞ –Ω–∞ —Ñ–æ–Ω—ñ, –¥–µ —É –≤–µ–ª–∏–∫—ñ–π –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø—Ä–∏—Å—É—Ç–Ω—ñ —Å—Ö–æ–∂—ñ –∑–∞ —Ñ–æ—Ä–º–æ—é –æ–±‚Äô—î–∫—Ç–∏, –Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ—à–Ω—ñ–π –¥–µ–Ω—å –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è —Å–∫–ª–∞–¥–Ω–∏–º –∑–∞–≤–¥–∞–Ω–Ω—è–º.–í—ñ–π—Å—å–∫–æ–≤–µ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –¥–ª—è –ø–æ—Å–∏–ª–µ–Ω–Ω—è —Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–∞—á–µ–Ω–Ω—è –≤—ñ–¥—ñ–≥—Ä–∞—î –∫–ª—é—á–æ–≤—É —Ä–æ–ª—å —É —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—ñ —Å—É—á–∞—Å–Ω–æ—ó –±–µ–∑–ø–µ–∫–∏. –¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—Ç—å—Å—è –¥–ª—è —Ç–æ–≥–æ, —â–æ–± –Ω–∞–¥–∞—Ç–∏ –±–µ–∑–ø–µ–∫–æ–≤–æ–≥–æ —Å–µ–Ω—Å—É –ø–æ–≤—Å—è–∫–¥–µ–Ω–Ω—ñ–π —Å–æ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ, –ø–æ–ª—è–º –±–∏—Ç–≤ —ñ –±–∞–≥–∞—Ç—å–æ–º —ñ–Ω—à–∏–º —Å—Ñ–µ—Ä–∞–º –∂–∏—Ç—Ç—î–¥—ñ—è–ª—å–Ω–æ—Å—Ç—ñ —Å—É—Å–ø—ñ–ª—å—Å—Ç–≤–∞.–£ —Å—Ç–∞—Ç—Ç—ñ –¥–æ—Å–ª—ñ–¥–∂—É—î—Ç—å—Å—è —Ä–æ–∑–≤–∏—Ç–æ–∫ –≤—ñ–π—Å—å–∫–æ–≤–æ-—Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É. –†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±‚Äô—î–∫—Ç—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –¥–ª—è —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –≤—ñ–π—Å—å–∫–æ–≤–∏–º–∏ –æ–ø–µ—Ä–∞—Ü—ñ—è–º–∏ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –ø–æ–∫–æ–ª—ñ–Ω–Ω—è, —â–æ –≤–∑–∞—î–º–æ–¥—ñ—é—Ç—å –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–º–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è–º–∏.–†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±‚Äô—î–∫—Ç—ñ–≤ –∑–∞—Å–æ–±–∞–º–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É —î –æ—Å–Ω–æ–≤–æ—é –±—ñ–ª—å—à–æ—Å—Ç—ñ –ø—Ä–æ–≥—Ä–∞–º–Ω–æ–≥–æ –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è —Ç–∞ –ø—Ä–æ–≥—Ä–∞–º —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É. –ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–æ, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–æ–≤—ñ—Ç–Ω—ñ—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É –±—É–¥–µ –∑–±—ñ–ª—å—à—É–≤–∞—Ç–∏—Å—è. –í–∏—è–≤–ª–µ–Ω–Ω—è –æ–±‚Äô—î–∫—Ç—ñ–≤ —î –≤–∞–∂–ª–∏–≤–∏–º —ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–º —É —Å—Ñ–µ—Ä–∞—Ö –æ–±–æ—Ä–æ–Ω–æ–∑–¥–∞—Ç–Ω–æ—Å—Ç—ñ, –±–µ–∑–ø–µ–∫–∏, –≤—ñ–π—Å—å–∫–æ–≤–æ—ó —Å–ø—Ä–∞–≤, —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç—É —Ç–æ—â–æ.–ü—Ä–æ–≤–µ–¥–µ–Ω–æ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –ë–ø–õ–ê —Ç–∞ –≤–∏–∑–Ω–∞—á–µ–Ω–æ —ó—Ö –æ–±–º–µ–∂–µ–Ω–Ω—è. –í —Å—Ç–∞—Ç—Ç—ñ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å. –í —Ä–æ–±–æ—Ç—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –º–µ—Ç–æ–¥ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è. –ü–æ—Ä–æ–≥–æ–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –æ–±—Ä–æ–±–ª—è—î —ñ –¥—ñ–ª–∏—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–∞ –æ–±–ª–∞—Å—Ç—ñ, –∑–≥—ñ–¥–Ω–æ –∑ –ø–æ—Ä–æ–≥–æ–≤–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏. –¢–∞–∫–∏–π –º–µ—Ç–æ–¥ –∑–∞–∑–≤–∏—á–∞–π –º–∞—î —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è, —è–∫—ñ –∑–∞–ª–µ–∂–∞—Ç—å –≤—ñ–¥ –¥–∞–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è. –ú–µ—Ç–æ–¥ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ–≥–æ –ø–æ—Ä–æ–≥—É —Å–µ–≥–º–µ–Ω—Ç—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —à–ª—è—Ö–æ–º –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ –æ–±–º–µ–∂–µ–Ω–Ω—è. –¢–∞–∫–∏–π –º–µ—Ç–æ–¥ –∑–∞–∑–≤–∏—á–∞–π –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è –≤ –±—ñ–Ω–∞—Ä–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è—Ö. –ù–∞–≤–µ–¥–µ–Ω–∞ —Ñ–æ—Ä–º—É–ª–∞ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º—ñ—Ç–∫–∏ –æ–±–ª–∞—Å—Ç—ñ –ø—ñ—Å–ª—è –≥—Ä–∞–Ω–∏—á–Ω–æ–≥–æ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–ª—è —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ–≥–æ –ø–æ—Ä–æ–≥—É —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è.
–ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–∏–π –∑—ñ—Ä, —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±‚Äô—î–∫—Ç—ñ–≤, –≤—ñ–π—Å—å–∫–æ–≤–∞ –≥–∞–ª—É–∑—å, –æ–±—Ä–æ–±–∫–∞ –∑–æ–±—Ä–∞–∂–µ–Ω—å, —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.
–°–ø–∏—Å–æ–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª1. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In {CVPR}, 2016. {ofBook} - Image Processing and Computer Vision. {URL}: http://surl.li/gppvv (–î–∞—Ç–∞ –∑–≤–µ—Ä–Ω–µ–Ω–Ω—è: 04.11.2022—Ä.). 2. –ë–æ–Ω–¥–∞—Ä, –Ü–ª–ª—è. –û–±—Ä–æ–±–∫–∞ –≤—ñ–¥–µ–æ –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —Ç–∞ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –æ–±'—î–∫—Ç—ñ–≤. 2021. 3. –ö–æ–º–ø'—é—Ç–µ—Ä–Ω–∏–π –∑—ñ—Ä. {URL}: http://surl.li/gppvp (–î–∞—Ç–∞ –∑–≤–µ—Ä–Ω–µ–Ω–Ω—è: 04.11.2022—Ä.). 4. –ö–æ—Ä–Ω—î–≤ –î. –í., –ú–∏–∫–∏—Ç–µ–Ω–∫–æ –í.–Ü. –ü–æ—à—É–∫ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤ –≤–∏—è–≤–ª–µ–Ω–Ω—è –±–µ–∑–ø—ñ–ª–æ—Ç–Ω–∏—Ö –ª—ñ—Ç–∞–ª—å–Ω–∏—Ö –∞–ø–∞—Ä–∞—Ç—ñ–≤. 2021. 5. –õ–µ—Å—é–∫ –ê.–ú, –Ø—Ü–∏—à–∏–Ω –°.–ü. –ö–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–∏–π –∑—ñ—Ä —Ç–∞ –π–æ–≥–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è. Editorial board. 2020, —Å.530. 6. –õ—ñ—Å–æ–≤–∏–π –í.–Æ. –°–∏—Å—Ç–µ–º–∞ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –∑–±—Ä–æ—ó –¥–ª—è –∫–∞–º–µ—Ä –≤—ñ–¥–µ–æ—Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –º–µ—Ç–æ–¥—ñ–≤ –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É. –ö–∏—ó–≤, 2018. 7. –ú–∞–∑—É—Ä –ú. –í. –ó–∞–¥–∞—á–∞ –≤–∏—è–≤–ª–µ–Ω–Ω—è —ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –æ–±'—î–∫—Ç—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –∑–≥–æ—Ä–∫—Ç–æ–≤–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂. –í–ù–¢–£, 2019. 8. –ú—ñ—Å—é—Ä–∞ –û.–°. –ü—Ä–æ–≥—Ä–∞–º–∞ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø–æ—à—É–∫ –∞–Ω–∞–ª–æ–≥—ñ–≤ —É –±–∞–∑—ñ –¥–∞–Ω–∏—Ö. –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–æ-–∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ–π–Ω—ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –≤ –æ—Å–≤—ñ—Ç—ñ ‚Ññ4. 2017. 9. –û–º–µ–ª—å—á–µ–Ω–∫–æ, –°. –û. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –æ–±—Ä–∞–∑—ñ–≤. –ì–û ¬´–Ñ–≤—Ä–æ–ø–µ–π—Å—å–∫–∞ –Ω–∞—É–∫–æ–≤–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞¬ª, 2021. 10. –†–æ–º–∞–Ω—á–∞, –ê. –ü., –ë–æ—Ä–∏—Å–æ–≤ –î.–í., –ü–æ–¥–æ—Ä–æ–∂–Ω—è–∫ –ê.–û. –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Å—É—á–∞—Å–Ω–∏—Ö —Å–∏—Å—Ç–µ–º –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É. –ß–µ—Ä–∫–∞—Å—å–∫–∏–π –¥–µ—Ä–∂–∞–≤–Ω–∏–π —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∏–π —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç, 2019. 11. –¢–µ—Ä–µ—â–µ–Ω–∫–æ, –í. –ú., –¢–µ—Ä–µ—â–µ–Ω–∫–æ –Ø.–í. –û–¥–∏–Ω –ø—ñ–¥—Ö—ñ–¥ –¥–æ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–Ω–∏—Ö –æ–±‚Äô—î–∫—Ç—ñ–≤ —É –∑–∞–¥–∞—á–∞—Ö –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É. –®—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç ‚Ññ2. 2016, —Å. 47-57.},
	pages = {46--52},
	number = {4},
	journaltitle = {–¢–ï–õ–ï–ö–û–ú–£–ù–Ü–ö–ê–¶–Ü–ô–ù–Ü –¢–ê –Ü–ù–§–û–†–ú–ê–¶–Ü–ô–ù–Ü –¢–ï–•–ù–û–õ–û–ì–Ü–á},
	author = {–ì–∞–Ω–≥–∞–ª–æ, –Ü. –ú. and –õ—ñ—Å–æ–≤–∏–π, –î. –û. and –ñ–µ–±–∫–∞, –í. –í.},
	urldate = {2026-02-09},
	date = {2022},
	langid = {ukrainian},
	file = {Full Text PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\DB2PLTUD\\–ì–∞–Ω–≥–∞–ª–æ et al. - 2022 - –†–û–ó–ü–Ü–ó–ù–ê–í–ê–ù–ù–Ø –û–ë‚Äô–Ñ–ö–¢–Ü–í –ó–ê –î–û–ü–û–ú–û–ì–û–Æ –¢–ï–•–ù–û–õ–û–ì–Ü–ô –ö–û–ú–ü‚Äô–Æ–¢–ï–†–ù–û–ì–û –ó–û–†–£.pdf:application/pdf},
}

@article{zhao_object_2019,
	title = {Object Detection With Deep Learning: A Review},
	volume = {30},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/document/8627998},
	doi = {10.1109/TNNLS.2018.2876865},
	shorttitle = {Object Detection With Deep Learning},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	pages = {3212--3232},
	number = {11},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	urldate = {2026-02-09},
	date = {2019-11},
	keywords = {Computer architecture, Deep learning, Feature extraction, neural network, Neural networks, object detection, Object detection, Task analysis, Training},
	file = {Submitted Version:C\:\\Users\\daniilsan\\Zotero\\storage\\ZSJIT5J4\\Zhao et al. - 2019 - Object Detection With Deep Learning A Review.pdf:application/pdf},
}

@inproceedings{redmon_you_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/CVPR.2016.91},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classiÔ¨Åers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {779--788},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2026-02-09},
	date = {2016-06},
	langid = {english},
	file = {PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\2K3GQS6C\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf:application/pdf},
}

@article{mopuri_cnn_2019,
	title = {{CNN} Fixations: An Unraveling Approach to Visualize the Discriminative Image Regions},
	volume = {28},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/document/8537979},
	doi = {10.1109/TIP.2018.2881920},
	shorttitle = {{CNN} Fixations},
	abstract = {Deep convolutional neural networks ({CNNs}) have revolutionized the computer vision research and have seen unprecedented adoption for multiple tasks, such as classification, detection, and caption generation. However, they offer little transparency into their inner workings and are often treated as black boxes that deliver excellent performance. In this paper, we aim at alleviating this opaqueness of {CNNs} by providing visual explanations for the network's predictions. Our approach can analyze a variety of {CNN}-based models trained for computer vision applications, such as object recognition and caption generation. Unlike the existing methods, we achieve this via unraveling the forward pass operation. The proposed method exploits feature dependencies across the layer hierarchy and uncovers the discriminative image locations that guide the network's predictions. We name these locations {CNN} fixations, loosely analogous to human eye fixations. Our approach is a generic method that requires no architectural changes, additional training, or gradient computation, and computes the important image locations ({CNN} fixations). We demonstrate through a variety of applications that our approach is able to localize the discriminative image locations across different network architectures, diverse vision tasks, and data modalities.},
	pages = {2116--2125},
	number = {5},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Mopuri, Konda Reddy and Garg, Utsav and Venkatesh Babu, R.},
	urldate = {2026-02-09},
	date = {2019-05},
	keywords = {{CNN} visualization, Computer architecture, Convolution, Explainable {AI}, label localization, Network architecture, Neurons, Task analysis, Training, visual explanations, Visualization, weakly supervised localization},
	file = {Submitted Version:C\:\\Users\\daniilsan\\Zotero\\storage\\AQA84DGB\\Mopuri et al. - 2019 - CNN Fixations An Unraveling Approach to Visualize the Discriminative Image Regions.pdf:application/pdf},
}

@article{khan_survey_2023,
	title = {A survey of the vision transformers and their {CNN}-transformer based variants},
	volume = {56},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-023-10595-0},
	doi = {10.1007/s10462-023-10595-0},
	abstract = {Vision transformers have become popular as a possible substitute to convolutional neural networks ({CNNs}) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity. However, they may suffer from limited generalization as they do not tend to model local correlation in images. Recently, in vision transformers hybridization of both the convolution operation and self-attention mechanism has emerged, to exploit both the local and global image representations. These hybrid vision transformers, also referred to as {CNN}-Transformer architectures, have demonstrated remarkable results in vision applications. Given the rapidly growing number of hybrid vision transformers, it has become necessary to provide a taxonomy and explanation of these hybrid architectures. This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers. Additionally, the key features of these architectures such as the attention mechanisms, positional embeddings, multi-scale processing, and convolution are also discussed. In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or {CNNs}, this survey uniquely emphasizes the emerging trend of hybrid vision transformers. By showcasing the potential of hybrid vision transformers to deliver exceptional performance across a range of computer vision tasks, this survey sheds light on the future directions of this rapidly evolving architecture.},
	pages = {2917--2970},
	number = {3},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Khan, Asifullah and Rauf, Zunaira and Sohail, Anabia and Khan, Abdul Rehman and Asif, Hifsa and Asif, Aqsa and Farooq, Umair},
	urldate = {2026-02-09},
	date = {2023-12-01},
	langid = {english},
	keywords = {Auto encoder, Channel boosting, Computer vision, Convolutional neural networks, Deep learning, Hybrid vision transformers, Image processing, Self-attention, Transformer},
	file = {Submitted Version:C\:\\Users\\daniilsan\\Zotero\\storage\\ADYPQIXW\\Khan et al. - 2023 - A survey of the vision transformers and their CNN-transformer based variants.pdf:application/pdf},
}

@inproceedings{filipiuk_comparing_2022,
	title = {Comparing Vision Transformers and Convolutional Nets for Safety Critical Systems},
	url = {https://ceur-ws.org/Vol-3087/paper_31.pdf},
	abstract = {Transformer based architectures like vision transformers ({ViTs}) are improving the state-of-the-art established by convolutional neural networks ({CNNs}) for computer vision tasks. Recent research shows that {ViTs} learn differently than {CNNs}, that provides an appealing choice to developers of safetycritical applications for redundant design. Moreover, {ViTs} have been shown to be robust to image perturbations. In this position paper, we analyze the properties of {ViTs} and compare them to {CNNs}. We create an ensemble of a {CNN} and a {ViT} and compare its performance to individual models. On the {ImageNet} benchmark, the ensemble shows minor improvements in accuracy relative to individual models. On the image corruption benchmark {ImageNet}-C, the ensemble shows up to 10\% improvement over the individual models, and generally performs as well as better of the two individual networks.},
	eventtitle = {Workshop on Artificial Intelligence Safety 2022 ({SafeAI} 2022)},
	number = {3087},
	publisher = {{CEUR}-{WS}},
	author = {Filipiuk, Micha≈Ç and Singh, Vasu},
	date = {2022-02-15},
	langid = {english},
	file = {PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\44GUXQF7\\Filipiuk and Singh - Comparing Vision Transformers and Convolutional Nets for Safety Critical Systems.pdf:application/pdf},
}

@article{mao_yolo_2025,
	title = {{YOLO} Object Detection for Real-Time Fabric Defect Inspection in the Textile Industry: A Review of {YOLOv}1 to {YOLOv}11},
	volume = {25},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/25/7/2270},
	doi = {10.3390/s25072270},
	shorttitle = {{YOLO} Object Detection for Real-Time Fabric Defect Inspection in the Textile Industry},
	abstract = {Automated fabric defect detection is crucial for improving quality control, reducing manual labor, and optimizing efficiency in the textile industry. ...},
	number = {7},
	journaltitle = {Sensors},
	publisher = {publisher},
	author = {Mao, Makara and Hong, Min},
	urldate = {2026-02-09},
	date = {2025-04-03},
	langid = {english},
	keywords = {convolutional neural networks, deep learning in textiles, fabric detection, quality control, real-time defect detection, textile industry, {YOLO} variants},
	file = {Full Text PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\2RLSLG7Z\\Mao and Hong - 2025 - YOLO Object Detection for Real-Time Fabric Defect Inspection in the Textile Industry A Review of YO.pdf:application/pdf},
}

@inproceedings{chaitra_text_2023,
	location = {Singapore},
	title = {Text Detection and Recognition from the Scene Images Using {RCNN} and {EasyOCR}},
	isbn = {978-981-99-3761-5},
	doi = {10.1007/978-981-99-3761-5_8},
	abstract = {Detecting text location and recognizing them from scene images remains one of the most challenging and enduring research problems in the field of computer vision. Over the past few decades, various researchers working hard to increase the accuracy of text recognition in scene images by considering several challenges. Because of this, there is a high need for commercial text recognizers for natural scenes. Conventional optical character recognition ({OCR}) demands clean backgrounds, crisp layouts, and neat text, which are frequently not met by natural scene images. A major challenge in scene text recognition is its orientation. There are various orientations like horizontal, vertical, diagonal and off-diagonal. Sometimes these orientations are in curved nature than straight. Another challenge is texts are embedded with un-uniform backgrounds and complicated environments. The extraction of such text is harrowing because of noisy backgrounds, diverse fonts, and text sizes. In this research, a comprehensive solution for detecting text using Faster {RCNN} and {EasyOCR} for text recognition is used to increase accuracy. The proposed algorithm is tested on benchmark datasets such as {ICDAR}13 and {ICDAR}15. F-score improves by 2.1 and 1.4\% for detection on {ICDAR}13 and 15 datasets, respectively. Also noticed that 1.8\% of improvement in recognition accuracy.},
	pages = {75--85},
	booktitle = {{IOT} with Smart Systems},
	publisher = {Springer Nature},
	author = {Chaitra, Y. L. and Roopa, M. J. and Gopalakrishna, M. T. and Swetha, M. D. and Aditya, C. R.},
	editor = {Choudrie, Jyoti and Mahalle, Parikshit N. and Perumal, Thinagaran and Joshi, Amit},
	date = {2023},
	langid = {english},
	keywords = {{EasyOCR}, {ICDAR}13, {ICDAR}15, {RCNN}, Text detection, Text recognition},
}

@inproceedings{smelyakov_effectiveness_2021,
	location = {Kharkiv, Ukraine},
	title = {Effectiveness of Modern Text Recognition Solutions and Tools for Common Data Sources},
	url = {https://ceur-ws.org/Vol-2870/paper15.pdf},
	abstract = {In the article features of functioning of the most common optical character recognition ({OCR}) tools {EasyOCR} and {TesserOCR} are considered; experimental analysis of results of work of these {OCR} is given for the most widespread data sources, such as electronic text document, internet resource, and banner; based on analysis of the experiment results from the comparative analysis of considered {OCRs} by time and accuracy was made; effective algorithm of using an {OCR} and recommendations for their application was offered for not-distorted data, for slightly and highly distorted data.},
	eventtitle = {5th International Conference on Computational Linguistics and Intelligent Systems},
	publisher = {{CEUR}-{WS}.org},
	author = {Smelyakov, Kirill and Chupryna, Anastasiya and Darahan, Dmytro and Midina, Serhii},
	date = {2021-04-22},
	langid = {english},
	file = {PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\4YGVYT9E\\Smelyakov et al. - Effectiveness of Modern Text Recognition Solutions and Tools for Common Data Sources.pdf:application/pdf},
}

@article{li_trocr_2023,
	title = {{TrOCR}: Transformer-Based Optical Character Recognition with Pre-trained Models},
	volume = {37},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26538},
	doi = {10.1609/aaai.v37i11.26538},
	shorttitle = {{TrOCR}},
	abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on {CNN} for image understanding and {RNN} for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely {TrOCR}, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The {TrOCR} model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the {TrOCR} model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The {TrOCR} models and code are publicly available at https://aka.ms/trocr.},
	pages = {13094--13102},
	number = {11},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
	urldate = {2026-02-09},
	date = {2023-06-26},
	langid = {english},
	keywords = {{CV}: Language and Vision},
	file = {Full Text PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\FNIFKTM9\\Li et al. - 2023 - TrOCR Transformer-Based Optical Character Recognition with Pre-trained Models.pdf:application/pdf},
}

@article{moussaoui_enhancing_2024,
	title = {Enhancing automated vehicle identification by integrating {YOLO} v8 and {OCR} techniques for high-precision license plate detection and recognition},
	volume = {14},
	rights = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-65272-1},
	doi = {10.1038/s41598-024-65272-1},
	abstract = {Vehicle identification systems are vital components that enable many aspects of contemporary life, such as safety, trade, transit, and law enforcement. They improve community and individual well-being by increasing vehicle management, security, and transparency. These tasks entail locating and extracting license plates from images or video frames using computer vision and machine learning techniques, followed by recognizing the letters or digits on the plates. This paper proposes a new license plate detection and recognition method based on the deep learning {YOLO} v8 method, image processing techniques, and the {OCR} technique for text recognition. For this, the first step was the dataset creation, when gathering 270 images from the internet. Afterward, {CVAT} (Computer Vision Annotation Tool) was used to annotate the dataset, which is an open-source software platform made to make computer vision tasks easier to annotate and label images and videos. Subsequently, the newly released Yolo version, the Yolo v8, has been employed to detect the number plate area in the input image. Subsequently, after extracting the plate the k-means clustering algorithm, the thresholding techniques, and the opening morphological operation were used to enhance the image and make the characters in the license plate clearer before using {OCR}. The next step in this process is using the {OCR} technique to extract the characters. Eventually, a text file containing only the character reflecting the vehicle's country is generated. To ameliorate the efficiency of the proposed approach, several metrics were employed, namely precision, recall, F1-Score, and {CLA}. In addition, a comparison of the proposed method with existing techniques in the literature has been given. The suggested method obtained convincing results in both detection as well as recognition by obtaining an accuracy of 99\% in detection and 98\% in character recognition.},
	pages = {14389},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	publisher = {Nature Publishing Group},
	author = {Moussaoui, Hanae and Akkad, Nabil El and Benslimane, Mohamed and El-Shafai, Walid and Baihan, Abdullah and Hewage, Chaminda and Rathore, Rajkumar Singh},
	urldate = {2026-02-09},
	date = {2024-06-22},
	langid = {english},
	keywords = {Computer science, Information technology, Software},
	file = {Full Text PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\6WXHVXI7\\Moussaoui et al. - 2024 - Enhancing automated vehicle identification by integrating YOLO v8 and OCR techniques for high-precis.pdf:application/pdf},
}

@inproceedings{dong_cnn-based_2017,
	location = {London, {UK}},
	title = {A {CNN}-Based Approach for Automatic License Plate Recognition in the Wild},
	isbn = {978-1-901725-60-5},
	url = {http://www.bmva.org/bmvc/2017/papers/paper175/index.html},
	doi = {10.5244/C.31.175},
	abstract = {In this paper, we address automatic license plate recognition ({ALPR}) in the wild. Such an {ALPR} system takes an arbitrary image as input and outputs the recognized license plate numbers. In the detection stage, we adopt a cascade structure comprising of a fast region proposal network and a R-{CNN} network. The R-{CNN} network not only eliminates false alarms but also regresses corner positions for each detected plate. This allows us to estimate an afÔ¨Åne transformation matrix to rectify the extracted plates. In the recognition stage, we propose an innovative structure composed of parallel spatial transform networks and shared-weight recognizers. The system is trained and evaluated on a Chinese license plate dataset with over 18K images. Results show that our detector performs better than faster R-{CNN} ({VGG}) which is 1.5x slower in testing and 57x larger in model size. The recognizer is also signiÔ¨Åcantly better than existing solutions, reducing 57.5\% of the errors of a state-of-the-art character sequence encoding scheme.},
	eventtitle = {British Machine Vision Conference 2017},
	pages = {175},
	booktitle = {Procedings of the British Machine Vision Conference 2017},
	publisher = {British Machine Vision Association},
	author = {Dong, Meng and He, Dongliang and Luo, Chong and Liu, Dong and Zeng, Wenjun},
	urldate = {2026-02-09},
	date = {2017},
	langid = {english},
	file = {PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\QQPVMCBN\\Dong et al. - 2017 - A CNN-Based Approach for Automatic License Plate Recognition in the Wild.pdf:application/pdf},
}

@online{wwwtechpowerupcom_nvidia_2026,
	title = {{NVIDIA} Tesla K40c Specs},
	url = {https://www.techpowerup.com/gpu-specs/tesla-k40c.c2505},
	abstract = {{NVIDIA} {GK}180, 876 {MHz}, 2880 Cores, 240 {TMUs}, 48 {ROPs}, 12288 {MB} {GDDR}5, 1502 {MHz}, 384 bit},
	titleaddon = {{TechPowerUp}},
	author = {{www.techpowerup.com}},
	urldate = {2026-02-09},
	date = {2026-02-09},
	langid = {english},
	file = {Snapshot:C\:\\Users\\daniilsan\\Zotero\\storage\\FEWPSXXB\\tesla-k40c.html:text/html},
}

@inproceedings{laroca_robust_2018,
	title = {A Robust Real-Time Automatic License Plate Recognition Based on the {YOLO} Detector},
	url = {http://arxiv.org/abs/1802.09567},
	doi = {10.1109/IJCNN.2018.8489629},
	abstract = {Automatic License Plate Recognition ({ALPR}) has been a frequent topic of research due to many practical applications. However, many of the current solutions are still not robust in real-world situations, commonly depending on many constraints. This paper presents a robust and efficient {ALPR} system based on the state-of-the-art {YOLO} object detector. The Convolutional Neural Networks ({CNNs}) are trained and fine-tuned for each {ALPR} stage so that they are robust under different conditions (e.g., variations in camera, lighting, and background). Specially for character segmentation and recognition, we design a two-stage approach employing simple data augmentation tricks such as inverted License Plates ({LPs}) and flipped characters. The resulting {ALPR} approach achieved impressive results in two datasets. First, in the {SSIG} dataset, composed of 2,000 frames from 101 vehicle videos, our system achieved a recognition rate of 93.53\% and 47 Frames Per Second ({FPS}), performing better than both Sighthound and {OpenALPR} commercial systems (89.80\% and 93.03\%, respectively) and considerably outperforming previous results (81.80\%). Second, targeting a more realistic scenario, we introduce a larger public dataset, called {UFPR}-{ALPR} dataset, designed to {ALPR}. This dataset contains 150 videos and 4,500 frames captured when both camera and vehicles are moving and also contains different types of vehicles (cars, motorcycles, buses and trucks). In our proposed dataset, the trial versions of commercial systems achieved recognition rates below 70\%. On the other hand, our system performed better, with recognition rate of 78.33\% and 35 {FPS}.},
	pages = {1--10},
	booktitle = {2018 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Laroca, Rayson and Severo, Evair and Zanlorensi, Luiz A. and Oliveira, Luiz S. and Gon√ßalves, Gabriel Resende and Schwartz, William Robson and Menotti, David},
	urldate = {2026-02-09},
	date = {2018-07},
	eprinttype = {arxiv},
	eprint = {1802.09567 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\66T3WVEU\\Laroca et al. - 2018 - A Robust Real-Time Automatic License Plate Recognition Based on the YOLO Detector.pdf:application/pdf;Snapshot:C\:\\Users\\daniilsan\\Zotero\\storage\\TVIPFKV2\\1802.html:text/html},
}

@online{wwwtechpowerupcom_nvidia_2026-1,
	title = {{NVIDIA} {TITAN} Xp Specs},
	url = {https://www.techpowerup.com/gpu-specs/titan-xp.c2948},
	abstract = {{NVIDIA} {GP}102, 1582 {MHz}, 3840 Cores, 240 {TMUs}, 96 {ROPs}, 12288 {MB} {GDDR}5X, 1426 {MHz}, 384 bit},
	titleaddon = {{TechPowerUp}},
	author = {{www.techpowerup.com}},
	urldate = {2026-02-09},
	date = {2026-02-09},
	langid = {english},
	file = {Snapshot:C\:\\Users\\daniilsan\\Zotero\\storage\\WKRM6PP4\\titan-xp.html:text/html},
}

@article{koresh_impact_2024,
	title = {Impact of the Preprocessing Steps in Deep Learning-Based Image Classifications},
	volume = {47},
	issn = {2250-1754},
	url = {https://doi.org/10.1007/s40009-023-01372-2},
	doi = {10.1007/s40009-023-01372-2},
	abstract = {Deep learning softwares are designed using artificial neural networks for various applications by training and testing them with an appropriate dataset. The raw image samples available in the dataset may contain noisy and unclear information due to radiation, heat and poor lighting conditions. Therefore, the researchers are trying to filter and enhance such noisy images through preprocessing steps for providing a valid feature information to the neural network layers included in the deep learning software. However, there are certain claims that roam around the researchers such as an image may lose some useful information when it is not preprocessed with an appropriate filter or enhancement technique. Hence, the work reviews the efficacy of the methodologies that are designed with and without a preprocessing step. Also, the work summarizes the common reasons and statements highlighted by the researchers for using and avoiding the preprocessing steps on designing a deep learning approach. The study is conducted to provide a clarity toward the requirement and non-requirement of preprocessing step in a deep learning software.},
	pages = {645--647},
	number = {6},
	journaltitle = {National Academy Science Letters},
	shortjournal = {Natl. Acad. Sci. Lett.},
	author = {Koresh, H. James Deva},
	urldate = {2026-02-09},
	date = {2024-12-01},
	langid = {english},
	keywords = {Classification, Computation reduction, Feature processing, Filters, Image enhancement},
}

@online{opencv_team_opencv_nodate,
	title = {{OpenCV} - Open Computer Vision Library},
	url = {https://opencv.org/},
	abstract = {{OpenCV} provides a real-time optimized Computer Vision library, tools, and hardware. It also supports model execution for Machine Learning ({ML}) and Artificial Intelligence ({AI}).},
	titleaddon = {{OpenCV}},
	author = {{OpenCV team}},
	urldate = {2026-02-09},
	langid = {american},
	file = {Snapshot:C\:\\Users\\daniilsan\\Zotero\\storage\\5PR59MZ5\\opencv.org.html:text/html},
}

@software{mauder_galfardeskew_2026,
	title = {galfar/deskew},
	rights = {{MPL}-2.0},
	url = {https://github.com/galfar/deskew},
	abstract = {Deskew is a command line tool for deskewing scanned text documents. It uses Hough transform to detect "text lines" in the image. As an output, you get an image rotated so that the lines are horizontal.},
	author = {Mauder, Marek},
	urldate = {2026-02-09},
	date = {2026-02-01},
	note = {original-date: 2018-10-30T16:37:58Z},
	keywords = {delphi, deskewing, free-pascal, lazarus, object-pascal, pascal, scanning, utility},
}

@online{noauthor_gigabyte_2026,
	title = {{GIGABYTE} {RTX} 4070 {SUPER} {WINDFORCE} {OC} Specs},
	url = {https://www.techpowerup.com/gpu-specs/gigabyte-rtx-4070-super-windforce-oc.b11487},
	abstract = {{NVIDIA} {AD}104, 2505 {MHz}, 7168 Cores, 224 {TMUs}, 80 {ROPs}, 12288 {MB} {GDDR}6X, 1313 {MHz}, 192 bit},
	titleaddon = {{TechPowerUp}},
	urldate = {2026-02-09},
	date = {2026-02-09},
	langid = {english},
	file = {Snapshot:C\:\\Users\\daniilsan\\Zotero\\storage\\AC42SB5G\\gigabyte-rtx-4070-super-windforce-oc.html:text/html},
}

@software{noauthor_jaidedaieasyocr_2026,
	title = {{JaidedAI}/{EasyOCR}},
	rights = {Apache-2.0},
	url = {https://github.com/JaidedAI/EasyOCR},
	abstract = {Ready-to-use {OCR} with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.},
	publisher = {Jaided {AI}},
	urldate = {2026-02-09},
	date = {2026-02-09},
	note = {original-date: 2020-03-14T11:46:39Z},
	keywords = {cnn, crnn, data-mining, deep-learning, easyocr, image-processing, information-retrieval, lstm, machine-learning, ocr, optical-character-recognition, python, pytorch, scene-text, scene-text-recognition},
}

@online{python_software_foundation_re_nodate,
	title = {re ‚Äî Regular expression operations},
	url = {https://docs.python.org/3/library/re.html},
	abstract = {Source code: Lib/re/ This module provides regular expression matching operations similar to those found in Perl. Both patterns and strings to be searched can be Unicode strings ( str) as well as 8-...},
	titleaddon = {Python documentation},
	author = {{Python Software Foundation}},
	urldate = {2026-02-09},
	langid = {english},
	file = {Snapshot:C\:\\Users\\daniilsan\\Zotero\\storage\\MP2CKYUL\\re.html:text/html},
}

@online{riacom_autoria_nodate,
	title = {{AUTO}.{RIA}‚Ñ¢ ‚Äî –ê–≤—Ç–æ–±–∞–∑–∞—Ä ‚Ññ1, –∫—É–ø–∏—Ç–∏ —Ç–∞ –ø—Ä–æ–¥–∞—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–µ –∞–≤—Ç–æ –ª–µ–≥–∫–æ!},
	url = {https://auto.ria.com/uk/},
	abstract = {{AUTO}.{RIA} ‚Äî —à–≤–∏–¥–∫–∏–π –ø—Ä–æ–¥–∞–∂ —Ç–∞ –ª–µ–≥–∫–∞ –∫—É–ø—ñ–≤–ª—è –Ω–æ–≤–∏—Ö —ñ –≤–∂–∏–≤–∞–Ω–∏—Ö –∞–≤—Ç–æ–º–æ–±—ñ–ª—ñ–≤. –ê–≤—Ç–æ–±–∞–∑–∞—Ä, –Ω–∞ —è–∫–æ–º—É –ø—Ä–æ–¥–∞—î—Ç—å—Å—è 1 400 –∞–≤—Ç–æ–º–æ–±—ñ–ª—ñ–≤ —â–æ–¥–Ω—è. –ü–æ—à—É–∫ –ø–æ –±–∞–∑—ñ –æ–≥–æ–ª–æ—à–µ–Ω—å –ø—Ä–æ –ø—Ä–æ–¥–∞–∂ –∞–≤—Ç–æ–º–æ–±—ñ–ª—ñ–≤. –ö–∞—Ç–∞–ª–æ–≥–∏ –∞–≤—Ç–æ—Å–∞–ª–æ–Ω—ñ–≤ —ñ –°–¢–û –Ω–∞ –ê–≤—Ç–æ—Ä—ñ–∞.},
	titleaddon = {{AUTO}.{RIA}},
	author = {{RIA.com}},
	urldate = {2026-02-09},
	langid = {ukrainian},
}

@misc{DSTU_4278-2019,
	edition = {–í–∏–¥–∞–Ω–Ω—è –æ—Ñ—ñ—Ü—ñ–π–Ω–µ},
	title = {–î–æ—Ä–æ–∂–Ω—ñ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç. –ó–Ω–∞–∫–∏ –Ω–æ–º–µ—Ä–Ω—ñ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∏—Ö –∑–∞—Å–æ–±—ñ–≤. –ó–∞–≥–∞–ª—å–Ω—ñ –≤–∏–º–æ–≥–∏. –ü—Ä–∞–≤–∏–ª–∞ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è},
	number = {–î–°–¢–£ 4278:2019},
	author = {{–ù–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π –°—Ç–∞–Ω–¥–∞—Ä—Ç –£–∫—Ä–∞—ó–Ω–∏}},
	date = {2020},
	file = {dstu_42782019_dorozhniy_transport_znaki_nomerni_transportnik:C\:\\Users\\daniilsan\\Zotero\\storage\\NRVYY4XD\\dstu_42782019_dorozhniy_transport_znaki_nomerni_transportnik.pdf:application/pdf},
}

@inproceedings{__2025,
	location = {–º. –ö–∏—ó–≤, –£–∫—Ä–∞—ó–Ω–∞},
	title = {–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∏—Ö –∑–∞—Å–æ–±—ñ–≤ —Ç–∞ –Ω–æ–º–µ—Ä–Ω–∏—Ö –∑–Ω–∞–∫—ñ–≤ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –º–æ–¥–µ–ª—ñ —Å—ñ–º–µ–π—Å—Ç–≤–∞ Ultralitics {YOLO}},
	eventtitle = {–°—Ç—É–¥–µ–Ω—Ç—Å—å–∫–∞ –Ω–∞—É–∫–æ–≤–æ-–ø—Ä–∞–∫—Ç–∏—á–Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è ¬´–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω—ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó —Ç–∞ –∫—ñ–±–µ—Ä–±–µ–∑–ø–µ–∫–∞ –≤ —É–º–æ–≤–∞—Ö –≤–æ—î–Ω–Ω–æ–≥–æ —á–∞—Å—É¬ª},
	pages = {6--9},
	publisher = {–î–µ—Ä–∂–∞–≤–Ω–∏–π —Ç–æ—Ä–≥–æ–≤–µ–ª—å–Ω–æ-–µ–∫–æ–Ω–æ–º—ñ—á–Ω–∏–π —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç},
	author = {–°–∞–Ω–∂–∞—Ä–æ–≤, –î–∞–Ω—ñ—ñ–ª and –§—ñ–ª—ñ–º–æ–Ω–æ–≤–∞, –¢–µ—Ç—è–Ω–∞},
	date = {2025-04-23},
	langid = {ukrainian},
}

@article{everingham_pascal_2010,
	title = {The Pascal Visual Object Classes ({VOC}) challenge},
	volume = {88},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The Pascal Visual Object Classes ({VOC}) challenge is a benchmark in visual object category recognition and detection, providing the vision
and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised
annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.

This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification
and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object
or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history
of the challenge, and proposes directions for future improvement and extension.},
	pages = {303--338},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher and Winn, John and Zisserman, Andrew},
	date = {2010-06-01},
	file = {Full Text PDF:C\:\\Users\\daniilsan\\Zotero\\storage\\25EZ5HND\\Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) challenge.pdf:application/pdf},
}

@incollection{bishop_pattern_2006,
	title = {Pattern Recognition and Machine Learning},
	volume = {16},
	doi = {10.1117/1.2819119},
	pages = {140--155},
	booktitle = {Journal of Electronic Imaging},
	author = {Bishop, Christopher},
	date = {2006-01-01},
	note = {Journal Abbreviation: Journal of Electronic Imaging},
}

@article{levenshtein_binary_1965,
	title = {Binary codes capable of correcting deletions, insertions, and reversals},
	url = {https://api.semanticscholar.org/CorpusID:60827152},
	abstract = {Semantic Scholar extracted view of "Binary codes capable of correcting deletions, insertions, and reversals" by V. Levenshtein},
	journaltitle = {Soviet physics. Doklady},
	author = {Levenshtein, V.},
	urldate = {2026-02-10},
	date = {1965},
}

@software{jocher_ultralytics_2023,
	title = {Ultralytics {YOLO}},
	rights = {{AGPL}-3.0},
	url = {https://github.com/ultralytics/ultralytics},
	abstract = {Ultralytics {YOLO} üöÄ},
	version = {8.0.0},
	author = {Jocher, Glenn and Qiu, Jing and Chaurasia, Ayush},
	urldate = {2026-02-10},
	date = {2023-01},
	note = {original-date: 2022-09-11T16:39:45Z},
}

@software{fayez_sirfztesserocr_2026,
	title = {sirfz/tesserocr},
	rights = {{MIT}},
	url = {https://github.com/sirfz/tesserocr},
	abstract = {A Python wrapper for the tesseract-ocr {API}},
	author = {Fayez},
	urldate = {2026-02-10},
	date = {2026-02-05},
	note = {original-date: 2015-12-17T23:29:36Z},
	keywords = {cython, ocr, optical-character-recognition, python-library, tesseract},
}

@online{python_software_foundation_itertools_nodate,
	title = {itertools ‚Äî Functions creating iterators for efficient looping},
	url = {https://docs.python.org/3/library/itertools.html},
	abstract = {This module implements a number of iterator building blocks inspired by constructs from {APL}, Haskell, and {SML}. Each has been recast in a form suitable for Python. The module standardizes a core set...},
	titleaddon = {Python documentation},
	author = {{Python Software Foundation}},
	urldate = {2026-02-10},
	langid = {english},
	file = {Snapshot:C\:\\Users\\daniilsan\\Zotero\\storage\\2LLBCGSF\\itertools.html:text/html},
}
